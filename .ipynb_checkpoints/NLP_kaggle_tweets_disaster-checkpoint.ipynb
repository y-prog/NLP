{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "031f83cd",
   "metadata": {},
   "source": [
    "# <center>  TWEETS DISASTER DETECTION </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6c0da",
   "metadata": {},
   "source": [
    "## This task consists of building classification models classifying tweets indicatinga natural disaster   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c77281",
   "metadata": {},
   "source": [
    "#### import needed packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669b2e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer  \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.regularizers import l2, l1_l2\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f3e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83cd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet=pd.read_csv(r'train_tweets.csv')  # load train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785ed06",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba95beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a4bf3",
   "metadata": {},
   "source": [
    "### the above info and the first 5 rows of the dataframe provide us with the following information:\n",
    "* null values in 'keyword' and 'location' columns \n",
    "* special characters\n",
    "* occasional capital letters\n",
    "* 1/3 of null values in the'location' column\n",
    "* id column can be safely dropped as it gives no information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe31761",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_list=train_tweet.target.tolist()  # save target variable into a list\n",
    "train_tweet=train_tweet.drop(['id'], axis=1) # drop the id column\n",
    "train_tweet=train_tweet.drop(['target'], axis=1) # drop the target column, it will be added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffaacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet # dataframe without 'id' and 'target' columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2765448c",
   "metadata": {},
   "source": [
    "### in order to eliminate all special characters and obtain a clean text, all null values need to be converted to string as type-float values will generate errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46588023",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet['keyword'] = train_tweet['keyword'].fillna('key') # replace null values in 'keyword' with 'key'\n",
    "train_tweet['location'] = train_tweet['location'].fillna('loc') # replace null values in 'keyword' with 'loc'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed01f325",
   "metadata": {},
   "source": [
    "### Dataframe cleaning \n",
    "* convert letters to lowercase \n",
    "* remove all special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0248e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet=train_tweet.apply(lambda x: x.astype(str).str.lower())\n",
    "train_tweet=train_tweet.replace('\\[.*?\\]', ' ', regex=True)\n",
    "train_tweet= train_tweet.replace('%', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('\\w*\\d\\w*', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('@', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('\\\\?', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('#', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('\\\\\\http\\S+', ' ', regex= True)\n",
    "rain_tweet = train_tweet.replace('\\x89\\S+', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('[-()\\\"#/@;:<>{}`+=~|.!?,]', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('[‘’“”…]', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('åê', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('http\\S+', ' ', regex= True)\n",
    "train_tweet= train_tweet.replace('\\$', 's', regex= True)\n",
    "train_tweet= train_tweet.replace(\"[\\\"\\',]\", '', regex= True)\n",
    "train_tweet= train_tweet.replace('http', '', regex= True)\n",
    "\n",
    "\n",
    "train_tweet = train_tweet.replace(r'\\s+', ' ', regex=True)\n",
    "train_tweet.keyword = train_tweet.keyword.replace(r'\\s+', '', regex=True)\n",
    "\n",
    "train_tweet['target']=target_list # include target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4399e28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_keyword=dict(train_tweet.keyword.value_counts()) # word frequency dictionary\n",
    "print(dict_keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1025420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2b09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_loc=dict(train_tweet.location.value_counts()) # word frequency dictionary\n",
    "print([str(round((i/train_tweet.shape[0])*100,2))+' %' for i in dict_loc.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa9cbdf",
   "metadata": {},
   "source": [
    "'there are 7613 rows in the dataframe and 33.3% are NaN values (coverted to  locs). \n",
    "second most frequent value (usas) makes up for less than 1.5% of the entire location columns, therefore\n",
    "unless we can witness a strong correlation between the NaN values and the target variable, this column can be \n",
    "safely dropped'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b58c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'the id column can be safely dropped, since it gives us no insight'\n",
    "'the keyword column will be dropped as well since despite being relevant, its presence does not represent a real life scenario'\n",
    "'as in several real world cases this variable is not given'\n",
    "'before deciding what should be done with the location, the content of this variable should be carefully observed'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51415117",
   "metadata": {},
   "source": [
    "The above graph shows that the target variable is almost evenly distributed with respect to the location column and combined with the scarce presence of the remaining categories, this column can be safely dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d263a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet=train_tweet.drop('location', axis=1) # drop location column\n",
    "train_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b737e",
   "metadata": {},
   "source": [
    "'keyword' column investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a048be32",
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_frequency_dictionary=dict(train_tweet.keyword.value_counts())\n",
    "keyword_frequency_dictionary_keys=(list(keyword_frequency_dictionary.keys()))\n",
    "keyword_frequency_percentage=([(str(round((i/train_tweet.shape[0]*100),2))+' %') for i in keyword_frequency_dictionary.values()])\n",
    "keyword_frequency_percentage_dict=dict(zip(keyword_frequency_dictionary_keys, keyword_frequency_percentage))\n",
    "print(keyword_frequency_percentage_dict) # percentage of unique keyword values in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd16b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot( keyword_frequency_dictionary.values())\n",
    "plt.title('keyword distrubution boxplot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f256bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_first = dict(list(keyword_frequency_dictionary.items())[1:]) # remove most frequent occurrence in keyword ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2c7749",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dict(list(keyword_frequency_dictionary.items())).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0465cb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.bar([i for i in range(len(keyword_frequency_dictionary))], list(keyword_frequency_dictionary.values()), align='edge')\n",
    "plt.xticks(range(len(keyword_frequency_dictionary)), list(keyword_frequency_dictionary.keys()), rotation = 90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23059a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "train_tweet['target_mean'] = train_tweet.groupby('keyword')['target'].transform('mean')\n",
    "\n",
    "fig = plt.figure(figsize=(12, 72), dpi=256)\n",
    "\n",
    "sns.countplot(y=train_tweet.sort_values(by='target_mean', ascending=False)['keyword'],\n",
    "              hue=train_tweet.sort_values(by='target_mean', ascending=False)['target'], palette = \"bright\")\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "plt.legend(loc=1)\n",
    "plt.title('Keywords distribution with respect to the target variable')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "train_tweet.drop(columns=['target_mean'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a698e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "sns.countplot(y = train_tweet.keyword, order = train_tweet.keyword.value_counts().iloc[:50].index, palette='bright')\n",
    "\n",
    "plt.tick_params(axis='x', labelsize=15)\n",
    "plt.tick_params(axis='y', labelsize=15)\n",
    "\n",
    "plt.title('Top 50 keywords')\n",
    "plt.show()\n",
    "\n",
    "print(len(train_tweet.keyword.value_counts()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "most_frequent_keywords=list(dict(train_tweet.keyword.value_counts()[:50]).keys())\n",
    "\n",
    "split = list(train_tweet['text'].str.split(' ')) #.explode()\n",
    "\n",
    "key_loc =  [index for index,value in enumerate(list(train_tweet.keyword)) if 'key' in value]\n",
    "\n",
    "print(key_loc)\n",
    "\n",
    "keyword_list=[]\n",
    "for sentence_index in range(len(split)):\n",
    "    check =[item for item in split[sentence_index] if item in most_frequent_keywords]\n",
    "    keyword_list.append(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5181aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f396ffd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_keyword_list=[]\n",
    "for cell in range(train_tweet.shape[0]):\n",
    "        if 'key' in list(train_tweet.keyword)[cell] and len(keyword_list[cell]) !=0:\n",
    "            missing_keyword_list.append(keyword_list[cell])\n",
    "        elif 'key' in list(train_tweet.keyword)[cell] and len(keyword_list[cell]) ==0:\n",
    "            missing_keyword_list.append('key')\n",
    "        else:\n",
    "            missing_keyword_list.append(list(train_tweet.keyword)[cell])\n",
    "\n",
    "#print(missing_keyword_list)\n",
    "\n",
    "#train_tweet['missing_keyword_list'] = train_tweet['missing_keyword_list'].str.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e263415",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet['missing_keywords']=missing_keyword_list\n",
    "clean_missing_keywords = [', '.join(map(str, l)) for l in train_tweet['missing_keywords']  ]\n",
    "#train_tweet['missing_keywords']=train_tweet.missing_keywords.apply(lambda x: ''.join(str(x)))\n",
    "train_tweet['missing_keywords'] = train_tweet['missing_keywords'].astype(str)\n",
    "train_tweet['missing_keywords'] = train_tweet['missing_keywords'].replace( ['\\[',']'],'', regex=True)\n",
    "train_tweet['missing_keywords'] = train_tweet['missing_keywords'].replace(\"'\", \"\", regex=True)\n",
    "#train_tweet.applymap(lambda x: x.replace(\"'\", ' '))\n",
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859b8940",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_tweet['text'] = train_tweet['text'].str.replace(re.escape(string.punctuation), ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4c1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8097d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sentences_length=[len(i.split()) for i in train_tweet.text]\n",
    "\n",
    "train_tweet['nr_sentences']=text_sentences_length\n",
    "\n",
    "ones_list=[]\n",
    "\n",
    "#ones= [('1')*len(text_sentences_length)]\n",
    "ones_list.extend(1 for i in range(len(text_sentences_length)))\n",
    "\n",
    "plt.figure(figsize=(5,10))\n",
    "plt.boxplot((text_sentences_length))\n",
    "plt.scatter(ones_list, text_sentences_length, marker='o', facecolors='none', edgecolors='r')\n",
    "plt.show()\n",
    "plt.hist(text_sentences_length)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a51948a",
   "metadata": {},
   "source": [
    "### as we can see, the length of words is normally distributed, we will therefore add another variable to the dataframe containing this aspect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153efc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_length(sent_len):\n",
    "    if sent_len<=10:\n",
    "        return 'SHORT'\n",
    "    elif sent_len >= 21:\n",
    "        return 'LONG'\n",
    "    else:\n",
    "        return 'AVG'\n",
    "\n",
    "train_tweet['sentence_length'] = [encode_length(i) for i in train_tweet.nr_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d8b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f193384",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "train_tweet['tweet_without_stopwords'] = train_tweet['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "train_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e62b1f",
   "metadata": {},
   "source": [
    "### Text lemmatization and tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53fcf4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(text):\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    words = text.split()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return  ' '.join(words)\n",
    "train_tweet['text_lemmatized'] = train_tweet.tweet_without_stopwords.apply(lemmatize_words)\n",
    "\n",
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ffb5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet['keyword_text_length']= train_tweet['keyword']+' '+ \\\n",
    "                                    train_tweet['text_lemmatized']+' '+train_tweet['sentence_length']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b3afce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(column):\n",
    "    tokens = word_tokenize(column)\n",
    "    return [w for w in tokens if w.isalpha()]\n",
    "\n",
    "train_tweet['tokenized'] = train_tweet.keyword_text_length.apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a496e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f94266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
